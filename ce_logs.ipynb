{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-05T16:50:59.998471Z","iopub.execute_input":"2023-10-05T16:50:59.998941Z","iopub.status.idle":"2023-10-05T16:51:00.372001Z","shell.execute_reply.started":"2023-10-05T16:50:59.998900Z","shell.execute_reply":"2023-10-05T16:51:00.371126Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!rm -r /kaggle/working/transformer","metadata":{"execution":{"iopub.status.busy":"2023-10-05T16:51:00.403436Z","iopub.execute_input":"2023-10-05T16:51:00.404349Z","iopub.status.idle":"2023-10-05T16:51:01.404446Z","shell.execute_reply.started":"2023-10-05T16:51:00.404317Z","shell.execute_reply":"2023-10-05T16:51:01.403188Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-10-05T16:51:01.406721Z","iopub.execute_input":"2023-10-05T16:51:01.407618Z","iopub.status.idle":"2023-10-05T16:51:01.416117Z","shell.execute_reply.started":"2023-10-05T16:51:01.407585Z","shell.execute_reply":"2023-10-05T16:51:01.415096Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/venkyyuvy/transformer.git","metadata":{"execution":{"iopub.status.busy":"2023-10-05T16:51:01.417348Z","iopub.execute_input":"2023-10-05T16:51:01.417872Z","iopub.status.idle":"2023-10-05T16:51:03.375067Z","shell.execute_reply.started":"2023-10-05T16:51:01.417825Z","shell.execute_reply":"2023-10-05T16:51:03.373926Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Cloning into 'transformer'...\nremote: Enumerating objects: 115, done.\u001b[K\nremote: Counting objects: 100% (115/115), done.\u001b[K\nremote: Compressing objects: 100% (84/84), done.\u001b[K\nremote: Total 115 (delta 56), reused 85 (delta 30), pack-reused 0\u001b[K\nReceiving objects: 100% (115/115), 402.89 KiB | 4.03 MiB/s, done.\nResolving deltas: 100% (56/56), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/transformer/unet","metadata":{"execution":{"iopub.status.busy":"2023-10-05T16:51:03.378045Z","iopub.execute_input":"2023-10-05T16:51:03.378536Z","iopub.status.idle":"2023-10-05T16:51:03.385388Z","shell.execute_reply.started":"2023-10-05T16:51:03.378508Z","shell.execute_reply":"2023-10-05T16:51:03.384389Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/working/transformer/unet\n","output_type":"stream"}]},{"cell_type":"code","source":"from model import UNet\nfrom dataset import get_pet_dataloader\nimport pytorch_lightning as pl\n\nbatch_size = 64\n\n\nif __name__ == \"__main__\":\n    train_loader, test_loader = get_pet_dataloader(batch_size=batch_size)\n\n    unet = UNet(\n        loss_fn=\"ce\",\n        contract_method=\"sc\",\n        expand_method=\"tr\"\n    )\n    trainer = pl.Trainer(\n        max_epochs=40,\n        strategy='ddp_notebook',\n\n    )\n    trainer.fit(\n        model=unet,\n        train_dataloaders=train_loader,\n    )\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T16:51:03.386877Z","iopub.execute_input":"2023-10-05T16:51:03.387821Z","iopub.status.idle":"2023-10-05T17:15:13.413604Z","shell.execute_reply.started":"2023-10-05T16:51:03.387791Z","shell.execute_reply":"2023-10-05T17:15:13.411329Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (29) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"493b9e5b1ff649e1a604671ef35d1e46"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n  warning_cache.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train_loader, test_loader = get_pet_dataloader(\".\", batch_size=batch_size)\n\n    unet = UNet(\n        loss_fn=\"ce\",\n        contract_method=\"mp\",\n        expand_method=\"tr\"\n    )\n    trainer = pl.Trainer(\n        max_epochs=40,\n    )\n    trainer.fit(\n        model=unet,\n        train_dataloaders=train_loader,\n    )","metadata":{"execution":{"iopub.status.busy":"2023-10-05T17:15:13.416101Z","iopub.execute_input":"2023-10-05T17:15:13.416661Z","iopub.status.idle":"2023-10-05T17:39:11.475737Z","shell.execute_reply.started":"2023-10-05T17:15:13.416629Z","shell.execute_reply":"2023-10-05T17:39:11.474513Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Downloading https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz to OxfordPets/train/oxford-iiit-pet/images.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 791918971/791918971 [00:26<00:00, 29386576.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting OxfordPets/train/oxford-iiit-pet/images.tar.gz to OxfordPets/train/oxford-iiit-pet\nDownloading https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz to OxfordPets/train/oxford-iiit-pet/annotations.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 19173078/19173078 [00:01<00:00, 16193709.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting OxfordPets/train/oxford-iiit-pet/annotations.tar.gz to OxfordPets/train/oxford-iiit-pet\nDownloading https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz to OxfordPets/test/oxford-iiit-pet/images.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 791918971/791918971 [00:26<00:00, 29385287.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting OxfordPets/test/oxford-iiit-pet/images.tar.gz to OxfordPets/test/oxford-iiit-pet\nDownloading https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz to OxfordPets/test/oxford-iiit-pet/annotations.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 19173078/19173078 [00:01<00:00, 14895811.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting OxfordPets/test/oxford-iiit-pet/annotations.tar.gz to OxfordPets/test/oxford-iiit-pet\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (29) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ccee03cdc2349e0be5412b925721284"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n  warning_cache.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}